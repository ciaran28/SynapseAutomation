{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "bigDataCluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7077baf6-e6fc-4e59-af0b-206c154d689e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/4f1bc772-7792-4285-99d9-3463b8d7f994/resourceGroups/synapse-dev-rg/providers/Microsoft.Synapse/workspaces/synapsewsdeploychd/bigDataPools/bigDataCluster",
				"name": "bigDataCluster",
				"type": "Spark",
				"endpoint": "https://synapsewsdeploychd.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bigDataCluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Databricks notebook source\r\n",
					"\"\"\"Example of framework usage\"\"\"\r\n",
					"# If setting up scope with Keyvault --> Ensure you have RBAC to KV Admin From 'AzureDatabrics' to Key Vault.\r\n",
					"import random\r\n",
					"from acai_ml.core import Engine\r\n",
					"import pandas as pd\r\n",
					"from pydataset import data\r\n",
					"from pathlib import Path\r\n",
					"from dbkcore.core import trace\r\n",
					"from dbkcore.core import Log\r\n",
					"from dbkdev.data_steps import DataStep, DataStepDataframe\r\n",
					"from dbkdev.data_steps import apply_test\r\n",
					"from sklearn.model_selection import ParameterSampler\r\n",
					"from sklearn.utils.fixes import loguniform\r\n",
					"from pyspark.sql import functions as F\r\n",
					"import numpy as np\r\n",
					"from sklearn.model_selection import cross_val_score\r\n",
					"from sklearn import svm\r\n",
					"\r\n",
					"\r\n",
					"class Step_loadData(DataStep):\r\n",
					"    \"\"\"Load the defined dataset.\"\"\"\r\n",
					"\r\n",
					"    def test(self):\r\n",
					"        \"\"\"Apply data tests.\"\"\"\r\n",
					"        self.test_is_dataframe_empty(df=self.output_data.dataframe)\r\n",
					"        self.test_null_values(\r\n",
					"            cols=['Sepal.Length', 'Sepal.Width'],\r\n",
					"            df=self.output_data.dataframe\r\n",
					"        )\r\n",
					"\r\n",
					"    @apply_test\r\n",
					"    @trace\r\n",
					"    def initialize(self, name_dataset: str):\r\n",
					"        \"\"\"\r\n",
					"        Initialize the DataStep.\r\n",
					"\r\n",
					"        Parameters\r\n",
					"        ----------\r\n",
					"        name_dataset : str\r\n",
					"            Name of the dataset to load from pydataset package\r\n",
					"        \"\"\"\r\n",
					"        p_df = data(name_dataset)\r\n",
					"        p_df.columns = [c.replace('.', '') for c in p_df.columns]\r\n",
					"        dt = self.spark.createDataFrame(p_df)\r\n",
					"        self.set_output_data(dt)\r\n",
					"\r\n",
					"\r\n",
					"class Step_crossValidate(DataStep):\r\n",
					"    \"\"\"Run multiple models in parallel.\"\"\"\r\n",
					"\r\n",
					"    def test(self):\r\n",
					"        pass\r\n",
					"\r\n",
					"    @trace(attrs_refact=['appi_ik'])\r\n",
					"    def initialize(\r\n",
					"        self,\r\n",
					"        dt: DataStepDataframe,\r\n",
					"        pipeline_name: str,\r\n",
					"        appi_ik: str,\r\n",
					"        n_iter: int\r\n",
					"    ):\r\n",
					"        param_grid = {\r\n",
					"            'C': loguniform(1e0, 1e3),\r\n",
					"            'kernel': ['linear', 'rbf'],\r\n",
					"            'class_weight': ['balanced', None]\r\n",
					"        }\r\n",
					"        rng = np.random.RandomState(0)\r\n",
					"        param_list = list(\r\n",
					"            ParameterSampler(\r\n",
					"                param_grid,\r\n",
					"                n_iter=n_iter,\r\n",
					"                random_state=rng\r\n",
					"            )\r\n",
					"        )\r\n",
					"        # p_dt = Engine.get_instance().spark().createDataFrame(pd.DataFrame(param_list)).\\\r\n",
					"        #     withColumn('id', F.monotonically_increasing_id())\r\n",
					"        p_dt = self.spark.createDataFrame(pd.DataFrame(param_list)).\\\r\n",
					"            withColumn('id', F.monotonically_increasing_id())\r\n",
					"        dt_train = dt.dataframe.crossJoin(\r\n",
					"            p_dt\r\n",
					"        )\r\n",
					"\r\n",
					"        udf_schema = dt_train.select(\r\n",
					"            'id',\r\n",
					"            F.lit(0.0).alias('score')\r\n",
					"        ).schema\r\n",
					"\r\n",
					"        def pudf_train(dt_model):\r\n",
					"            param_id = dt_model['id'].unique()[0]\r\n",
					"            param_c = dt_model['C'].unique()[0]\r\n",
					"            param_class_weight = dt_model['class_weight'].unique()[0]\r\n",
					"            param_kernel = dt_model['kernel'].unique()[0]\r\n",
					"\r\n",
					"            logging_custom_dimensions = {\r\n",
					"                'id': str(param_id),\r\n",
					"                'C': str(param_c),\r\n",
					"                'class_weight': param_class_weight,\r\n",
					"                'kernel': param_kernel\r\n",
					"            }\r\n",
					"\r\n",
					"            Log(pipeline_name, appi_ik)\r\n",
					"\r\n",
					"            try:\r\n",
					"\r\n",
					"                # Raising randomly exception\r\n",
					"                if random.randint(0, 20) > 15:\r\n",
					"                    raise 'Random exception'\r\n",
					"\r\n",
					"                dt_x = dt_model[\r\n",
					"                    [\r\n",
					"                        'SepalLength',\r\n",
					"                        'SepalWidth',\r\n",
					"                        'PetalLength',\r\n",
					"                        'PetalWidth'\r\n",
					"                    ]\r\n",
					"                ]\r\n",
					"                y = dt_model['Species']\r\n",
					"                clf = svm.SVC(\r\n",
					"                    kernel=param_kernel,\r\n",
					"                    C=param_c,\r\n",
					"                    class_weight=param_class_weight,\r\n",
					"                    random_state=42\r\n",
					"                )\r\n",
					"                scores = cross_val_score(clf, dt_x, y, cv=5, scoring='f1_macro')\r\n",
					"                score = scores.mean()\r\n",
					"                dt_out = pd.DataFrame(\r\n",
					"                    {\r\n",
					"                        'id': [param_id],\r\n",
					"                        'score': [score]\r\n",
					"                    }\r\n",
					"                )\r\n",
					"                Log.get_instance().log_info(\"Training:success\", custom_dimension=logging_custom_dimensions)\r\n",
					"            except Exception:\r\n",
					"                Log.get_instance().log_error(\"Training:failed\", custom_dimension=logging_custom_dimensions)\r\n",
					"                dt_out = pd.DataFrame(\r\n",
					"                    {\r\n",
					"                        'id': [param_id],\r\n",
					"                        'score': [-1]\r\n",
					"                    }\r\n",
					"                )\r\n",
					"            return dt_out\r\n",
					"\r\n",
					"        '''\r\n",
					"        dt_model = dt_train.where(F.col('id') == 17179869184).toPandas()\r\n",
					"        '''\r\n",
					"        dt_cross_evals = dt_train.\\\r\n",
					"            groupBy(['id']).\\\r\n",
					"            applyInPandas(pudf_train, schema=udf_schema).\\\r\n",
					"            cache()\r\n",
					"        dt_cross_evals.count()\r\n",
					"        self.set_output_data(dt_cross_evals)\r\n",
					"\r\n",
					"\r\n",
					"Engine()\r\n",
					"Engine().get_instance().initialize_env()\r\n",
					"# pipeline_name = Path(__file__).stem\r\n",
					"pipeline_name = \"Remote Testing\"\r\n",
					"\r\n",
					"Engine().get_instance().initialize_logger(pipeline_name=pipeline_name, appi_ik_scope=\"AzureResourceSecrets\", appi_ik_secret=\"appi_ik\")\r\n",
					"# Engine().get_instance().spark().conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\r\n",
					"\r\n",
					"run_id = 'test_run_id'\r\n",
					"\r\n",
					"step_loadData = Step_loadData(\r\n",
					"    spark=Engine.get_instance().spark(),\r\n",
					"    run_id=run_id\r\n",
					")\r\n",
					"\r\n",
					"step_loadData.initialize(\r\n",
					"    name_dataset='iris'\r\n",
					")\r\n",
					"\r\n",
					"step_crossValidate = Step_crossValidate(\r\n",
					"    spark=Engine.get_instance().spark(),\r\n",
					"    run_id=run_id\r\n",
					")\r\n",
					"\r\n",
					"step_crossValidate.initialize(\r\n",
					"    dt=step_loadData.output_data,\r\n",
					"    pipeline_name=pipeline_name,\r\n",
					"    appi_ik=Engine().get_instance().appi_ik,\r\n",
					"    n_iter=1000\r\n",
					")\r\n",
					"\r\n",
					"step_crossValidate.output_data.dataframe.toPandas()\r\n",
					"\r\n",
					"print(step_crossValidate.output_data.dataframe.toPandas())\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}