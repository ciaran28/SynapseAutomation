{
	"name": "test",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "44d362a2-f273-4016-9c0d-d02ee0ba644e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\"\"\"Example of framework usage\"\"\"\n",
					"# If setting up scope with Keyvault --> Ensure you have RBAC to KV Admin From 'AzureDatabrics' to Key Vault.\n",
					"import random\n",
					"from acai_ml.core import Engine\n",
					"import pandas as pd\n",
					"from pydataset import data\n",
					"from pathlib import Path\n",
					"from dbkcore.core import trace\n",
					"from dbkcore.core import Log\n",
					"from dbkdev.data_steps import DataStep, DataStepDataframe\n",
					"from dbkdev.data_steps import apply_test\n",
					"from sklearn.model_selection import ParameterSampler\n",
					"from sklearn.utils.fixes import loguniform\n",
					"from pyspark.sql import functions as F\n",
					"import numpy as np\n",
					"from sklearn.model_selection import cross_val_score\n",
					"from sklearn import svm\n",
					"\n",
					"\n",
					"class Step_loadData(DataStep):\n",
					"    \"\"\"Load the defined dataset.\"\"\"\n",
					"\n",
					"    def test(self):\n",
					"        \"\"\"Apply data tests.\"\"\"\n",
					"        self.test_is_dataframe_empty(df=self.output_data.dataframe)\n",
					"        self.test_null_values(\n",
					"            cols=['Sepal.Length', 'Sepal.Width'],\n",
					"            df=self.output_data.dataframe\n",
					"        )\n",
					"\n",
					"    @apply_test\n",
					"    @trace\n",
					"    def initialize(self, name_dataset: str):\n",
					"        \"\"\"\n",
					"        Initialize the DataStep.\n",
					"\n",
					"        Parameters\n",
					"        ----------\n",
					"        name_dataset : str\n",
					"            Name of the dataset to load from pydataset package\n",
					"        \"\"\"\n",
					"        p_df = data(name_dataset)\n",
					"        p_df.columns = [c.replace('.', '') for c in p_df.columns]\n",
					"        dt = self.spark.createDataFrame(p_df)\n",
					"        self.set_output_data(dt)\n",
					"\n",
					"\n",
					"class Step_crossValidate(DataStep):\n",
					"    \"\"\"Run multiple models in parallel.\"\"\"\n",
					"\n",
					"    def test(self):\n",
					"        pass\n",
					"\n",
					"    @trace(attrs_refact=['appi_ik'])\n",
					"    def initialize(\n",
					"        self,\n",
					"        dt: DataStepDataframe,\n",
					"        pipeline_name: str,\n",
					"        appi_ik: str,\n",
					"        n_iter: int\n",
					"    ):\n",
					"        param_grid = {\n",
					"            'C': loguniform(1e0, 1e3),\n",
					"            'kernel': ['linear', 'rbf'],\n",
					"            'class_weight': ['balanced', None]\n",
					"        }\n",
					"        rng = np.random.RandomState(0)\n",
					"        param_list = list(\n",
					"            ParameterSampler(\n",
					"                param_grid,\n",
					"                n_iter=n_iter,\n",
					"                random_state=rng\n",
					"            )\n",
					"        )\n",
					"        # p_dt = Engine.get_instance().spark().createDataFrame(pd.DataFrame(param_list)).\\\n",
					"        #     withColumn('id', F.monotonically_increasing_id())\n",
					"        p_dt = self.spark.createDataFrame(pd.DataFrame(param_list)).\\\n",
					"            withColumn('id', F.monotonically_increasing_id())\n",
					"        dt_train = dt.dataframe.crossJoin(\n",
					"            p_dt\n",
					"        )\n",
					"\n",
					"        udf_schema = dt_train.select(\n",
					"            'id',\n",
					"            F.lit(0.0).alias('score')\n",
					"        ).schema\n",
					"\n",
					"        def pudf_train(dt_model):\n",
					"            param_id = dt_model['id'].unique()[0]\n",
					"            param_c = dt_model['C'].unique()[0]\n",
					"            param_class_weight = dt_model['class_weight'].unique()[0]\n",
					"            param_kernel = dt_model['kernel'].unique()[0]\n",
					"\n",
					"            logging_custom_dimensions = {\n",
					"                'id': str(param_id),\n",
					"                'C': str(param_c),\n",
					"                'class_weight': param_class_weight,\n",
					"                'kernel': param_kernel\n",
					"            }\n",
					"\n",
					"            Log(pipeline_name, appi_ik)\n",
					"\n",
					"            try:\n",
					"\n",
					"                # Raising randomly exception\n",
					"                if random.randint(0, 20) > 15:\n",
					"                    raise 'Random exception'\n",
					"\n",
					"                dt_x = dt_model[\n",
					"                    [\n",
					"                        'SepalLength',\n",
					"                        'SepalWidth',\n",
					"                        'PetalLength',\n",
					"                        'PetalWidth'\n",
					"                    ]\n",
					"                ]\n",
					"                y = dt_model['Species']\n",
					"                clf = svm.SVC(\n",
					"                    kernel=param_kernel,\n",
					"                    C=param_c,\n",
					"                    class_weight=param_class_weight,\n",
					"                    random_state=42\n",
					"                )\n",
					"                scores = cross_val_score(clf, dt_x, y, cv=5, scoring='f1_macro')\n",
					"                score = scores.mean()\n",
					"                dt_out = pd.DataFrame(\n",
					"                    {\n",
					"                        'id': [param_id],\n",
					"                        'score': [score]\n",
					"                    }\n",
					"                )\n",
					"                Log.get_instance().log_info(\"Training:success\", custom_dimension=logging_custom_dimensions)\n",
					"            except Exception:\n",
					"                Log.get_instance().log_error(\"Training:failed\", custom_dimension=logging_custom_dimensions)\n",
					"                dt_out = pd.DataFrame(\n",
					"                    {\n",
					"                        'id': [param_id],\n",
					"                        'score': [-1]\n",
					"                    }\n",
					"                )\n",
					"            return dt_out\n",
					"\n",
					"        '''\n",
					"        dt_model = dt_train.where(F.col('id') == 17179869184).toPandas()\n",
					"        '''\n",
					"        dt_cross_evals = dt_train.\\\n",
					"            groupBy(['id']).\\\n",
					"            applyInPandas(pudf_train, schema=udf_schema).\\\n",
					"            cache()\n",
					"        dt_cross_evals.count()\n",
					"        self.set_output_data(dt_cross_evals)\n",
					"\n",
					"\n",
					"Engine()\n",
					"Engine().get_instance().initialize_env()\n",
					"# pipeline_name = Path(__file__).stem\n",
					"pipeline_name = \"Remote Testing\"\n",
					"\n",
					"Engine().get_instance().initialize_logger(pipeline_name=pipeline_name, appi_ik_scope=\"AzureResourceSecrets\", appi_ik_secret=\"appi_ik\")\n",
					"# Engine().get_instance().spark().conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
					"\n",
					"run_id = 'test_run_id'\n",
					"\n",
					"step_loadData = Step_loadData(\n",
					"    spark=Engine.get_instance().spark(),\n",
					"    run_id=run_id\n",
					")\n",
					"\n",
					"step_loadData.initialize(\n",
					"    name_dataset='iris'\n",
					")\n",
					"\n",
					"step_crossValidate = Step_crossValidate(\n",
					"    spark=Engine.get_instance().spark(),\n",
					"    run_id=run_id\n",
					")\n",
					"\n",
					"step_crossValidate.initialize(\n",
					"    dt=step_loadData.output_data,\n",
					"    pipeline_name=pipeline_name,\n",
					"    appi_ik=Engine().get_instance().appi_ik,\n",
					"    n_iter=1000\n",
					")\n",
					"\n",
					"step_crossValidate.output_data.dataframe.toPandas()\n",
					"\n",
					"print(step_crossValidate.output_data.dataframe.toPandas())\n",
					""
				],
				"execution_count": null
			}
		]
	}
}