{
	"name": "remote_analysis",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "bigDataCluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a68dcc53-3b14-433c-802d-c19d6bde783f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/4f1bc772-7792-4285-99d9-3463b8d7f994/resourceGroups/synapse-dev-rg/providers/Microsoft.Synapse/workspaces/synapsewsdeploychd/bigDataPools/bigDataCluster",
				"name": "bigDataCluster",
				"type": "Spark",
				"endpoint": "https://synapsewsdeploychd.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bigDataCluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%pip uninstall protobuf -y\r\n",
					"%pip uninstall protobuf -y\r\n",
					"%pip uninstall protobuf -y\r\n",
					"%pip uninstall protobuf -y\r\n",
					"%pip uninstall protobuf -y\r\n",
					"%pip uninstall protobuf -y\r\n",
					"\r\n",
					"%pip install protobuf==3.20.1\r\n",
					"\r\n",
					"%pip uninstall six -y\r\n",
					"%pip install six==1.15.0\r\n",
					"\r\n",
					"%pip uninstall typing-extension -y\r\n",
					"%pip install typing-extensions==3.7.4.3\r\n",
					""
				],
				"attachments": null,
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"\"\"\"Example of framework usage\"\"\"\n",
					"# If setting up scope with Keyvault --> Ensure you have RBAC to KV Admin From 'AzureDatabrics' to Key Vault.\n",
					"import random\n",
					"from acai_ml.core import Engine\n",
					"import pandas as pd\n",
					"from pydataset import data\n",
					"from pathlib import Path\n",
					"from dbkcore.core import trace\n",
					"from dbkcore.core import Log\n",
					"from dbkdev.data_steps import DataStep, DataStepDataframe\n",
					"from dbkdev.data_steps import apply_test\n",
					"from sklearn.model_selection import ParameterSampler\n",
					"from sklearn.utils.fixes import loguniform\n",
					"from pyspark.sql import functions as F\n",
					"import numpy as np\n",
					"from sklearn.model_selection import cross_val_score\n",
					"from sklearn import svm\n",
					"\n",
					"\n",
					"class Step_loadData(DataStep):\n",
					"    \"\"\"Load the defined dataset.\"\"\"\n",
					"\n",
					"    def test(self):\n",
					"        \"\"\"Apply data tests.\"\"\"\n",
					"        self.test_is_dataframe_empty(df=self.output_data.dataframe)\n",
					"        self.test_null_values(\n",
					"            cols=['Sepal.Length', 'Sepal.Width'],\n",
					"            df=self.output_data.dataframe\n",
					"        )\n",
					"\n",
					"    @apply_test\n",
					"    @trace\n",
					"    def initialize(self, name_dataset: str):\n",
					"        \"\"\"\n",
					"        Initialize the DataStep.\n",
					"\n",
					"        Parameters\n",
					"        ----------\n",
					"        name_dataset : str\n",
					"            Name of the dataset to load from pydataset package\n",
					"        \"\"\"\n",
					"        p_df = data(name_dataset)\n",
					"        p_df.columns = [c.replace('.', '') for c in p_df.columns]\n",
					"        dt = self.spark.createDataFrame(p_df)\n",
					"        self.set_output_data(dt)\n",
					"\n",
					"\n",
					"class Step_crossValidate(DataStep):\n",
					"    \"\"\"Run multiple models in parallel.\"\"\"\n",
					"\n",
					"    def test(self):\n",
					"        pass\n",
					"\n",
					"    @trace(attrs_refact=['appi_ik'])\n",
					"    def initialize(\n",
					"        self,\n",
					"        dt: DataStepDataframe,\n",
					"        pipeline_name: str,\n",
					"        appi_ik: str,\n",
					"        n_iter: int\n",
					"    ):\n",
					"        param_grid = {\n",
					"            'C': loguniform(1e0, 1e3),\n",
					"            'kernel': ['linear', 'rbf'],\n",
					"            'class_weight': ['balanced', None]\n",
					"        }\n",
					"        rng = np.random.RandomState(0)\n",
					"        param_list = list(\n",
					"            ParameterSampler(\n",
					"                param_grid,\n",
					"                n_iter=n_iter,\n",
					"                random_state=rng\n",
					"            )\n",
					"        )\n",
					"        # p_dt = Engine.get_instance().spark().createDataFrame(pd.DataFrame(param_list)).\\\n",
					"        #     withColumn('id', F.monotonically_increasing_id())\n",
					"        p_dt = self.spark.createDataFrame(pd.DataFrame(param_list)).\\\n",
					"            withColumn('id', F.monotonically_increasing_id())\n",
					"        dt_train = dt.dataframe.crossJoin(\n",
					"            p_dt\n",
					"        )\n",
					"\n",
					"        udf_schema = dt_train.select(\n",
					"            'id',\n",
					"            F.lit(0.0).alias('score')\n",
					"        ).schema\n",
					"\n",
					"        def pudf_train(dt_model):\n",
					"            param_id = dt_model['id'].unique()[0]\n",
					"            param_c = dt_model['C'].unique()[0]\n",
					"            param_class_weight = dt_model['class_weight'].unique()[0]\n",
					"            param_kernel = dt_model['kernel'].unique()[0]\n",
					"\n",
					"            logging_custom_dimensions = {\n",
					"                'id': str(param_id),\n",
					"                'C': str(param_c),\n",
					"                'class_weight': param_class_weight,\n",
					"                'kernel': param_kernel\n",
					"            }\n",
					"\n",
					"            Log(pipeline_name, appi_ik)\n",
					"\n",
					"            try:\n",
					"\n",
					"                # Raising randomly exception\n",
					"                if random.randint(0, 20) > 15:\n",
					"                    raise 'Random exception'\n",
					"\n",
					"                dt_x = dt_model[\n",
					"                    [\n",
					"                        'SepalLength',\n",
					"                        'SepalWidth',\n",
					"                        'PetalLength',\n",
					"                        'PetalWidth'\n",
					"                    ]\n",
					"                ]\n",
					"                y = dt_model['Species']\n",
					"                clf = svm.SVC(\n",
					"                    kernel=param_kernel,\n",
					"                    C=param_c,\n",
					"                    class_weight=param_class_weight,\n",
					"                    random_state=42\n",
					"                )\n",
					"                scores = cross_val_score(clf, dt_x, y, cv=5, scoring='f1_macro')\n",
					"                score = scores.mean()\n",
					"                dt_out = pd.DataFrame(\n",
					"                    {\n",
					"                        'id': [param_id],\n",
					"                        'score': [score]\n",
					"                    }\n",
					"                )\n",
					"                Log.get_instance().log_info(\"Training:success\", custom_dimension=logging_custom_dimensions)\n",
					"            except Exception:\n",
					"                Log.get_instance().log_error(\"Training:failed\", custom_dimension=logging_custom_dimensions)\n",
					"                dt_out = pd.DataFrame(\n",
					"                    {\n",
					"                        'id': [param_id],\n",
					"                        'score': [-1]\n",
					"                    }\n",
					"                )\n",
					"            return dt_out\n",
					"\n",
					"        '''\n",
					"        dt_model = dt_train.where(F.col('id') == 17179869184).toPandas()\n",
					"        '''\n",
					"        dt_cross_evals = dt_train.\\\n",
					"            groupBy(['id']).\\\n",
					"            applyInPandas(pudf_train, schema=udf_schema).\\\n",
					"            cache()\n",
					"        dt_cross_evals.count()\n",
					"        self.set_output_data(dt_cross_evals)\n",
					"\n",
					"\n",
					"Engine()\n",
					"Engine().get_instance().initialize_env()\n",
					"# pipeline_name = Path(__file__).stem\n",
					"pipeline_name = \"Remote Testing\"\n",
					"\n",
					"Engine().get_instance().initialize_logger(pipeline_name=pipeline_name, appi_ik_scope=\"AzureResourceSecrets\", appi_ik_secret=\"appi_ik\")\n",
					"# Engine().get_instance().spark().conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
					"\n",
					"run_id = 'test_run_id'\n",
					"\n",
					"step_loadData = Step_loadData(\n",
					"    spark=Engine.get_instance().spark(),\n",
					"    run_id=run_id\n",
					")\n",
					"\n",
					"step_loadData.initialize(\n",
					"    name_dataset='iris'\n",
					")\n",
					"\n",
					"step_crossValidate = Step_crossValidate(\n",
					"    spark=Engine.get_instance().spark(),\n",
					"    run_id=run_id\n",
					")\n",
					"\n",
					"step_crossValidate.initialize(\n",
					"    dt=step_loadData.output_data,\n",
					"    pipeline_name=pipeline_name,\n",
					"    appi_ik=Engine().get_instance().appi_ik,\n",
					"    n_iter=1000\n",
					")\n",
					"\n",
					"step_crossValidate.output_data.dataframe.toPandas()\n",
					"\n",
					"print(step_crossValidate.output_data.dataframe.toPandas())"
				],
				"attachments": null,
				"execution_count": 19
			}
		]
	}
}