{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = os.path.dirname(__file__)\n",
    "new_file = os.path.join(cur_path, 'Development.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data to be written \n",
    "dictionary_json = { \n",
    "    \"SubscriptionId\": \"4f1bc772-7792-4285-99d9-3463b8d7f994\",\n",
    "    \"Location\": \"uksouth\", \n",
    "    \"TemplateParamFilePath\":\"Infrastructure/DBX_CICD_Deployment/Bicep_Params/Development/Bicep.parameters.json\",\n",
    "    \"TemplateFilePath\":\"Infrastructure/DBX_CICD_Deployment/Main_DBX_CICD.bicep\",\n",
    "    \"AZURE_DATABRICKS_APP_ID\": \"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d\",\n",
    "    \"MANAGEMENT_RESOURCE_ENDPOINT\": \"https://management.core.windows.net/\",\n",
    "    \"Clusters\": [\n",
    "        {\n",
    "            \"cluster_name\": \"dbx-sp-cluster\",\n",
    "            \"spark_version\": \"10.4.x-scala2.12\",\n",
    "            \"node_type_id\": \"Standard_D3_v2\",\n",
    "            \"spark_conf\": {},\n",
    "            \"autotermination_minutes\": 30,\n",
    "            \"runtime_engine\": \"STANDARD\",\n",
    "            \"autoscale\": {\n",
    "                \"min_workers\": 2,\n",
    "                \"max_workers\": 4\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"cluster_name\": \"dbx-sp-cluster2\",\n",
    "            \"spark_version\": \"10.4.x-scala2.12\",\n",
    "            \"node_type_id\": \"Standard_D3_v2\",\n",
    "            \"spark_conf\": {},\n",
    "            \"autotermination_minutes\": 30,\n",
    "            \"runtime_engine\": \"STANDARD\",\n",
    "            \"autoscale\": {\n",
    "                \"min_workers\": 2,\n",
    "                \"max_workers\": 4\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "} \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is invoked when IPYNB is converted to .py file in Yaml DevOps\n",
    "Do not run the cells below in Synapse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = os.path.dirname(__file__)\n",
    "new_file = os.path.join(cur_path, 'Development.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the file path does not include a special marker from the DevOps Agent, then dont run\n",
    "# Check to see if it even \n",
    "with open(new_file, \"w\") as outfile:\n",
    "    json.dump(dictionary_json, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature Merge To Main\n",
    "2. Yaml Triggers\n",
    "3. Bash Script points to ipynb file, and converts to py, using nb convert. Development.py is overwritten (Development.py will be destroyed in time)\n",
    "4. Bash Script points to Development.py and runs it - which updates Development.json\n",
    "\n",
    "\n",
    "above is flaw\n",
    "\n",
    "1. The person works in their branch\n",
    "2. They commit to save their keep their changes saved\n",
    "3. When happy, they publish (do we really need to have merge request)\n",
    "4. The publish branch is updated\n",
    "5. Trigger linked to the publish branch is set in motion \n",
    "6. Synapse List Notebooks is run from bash. Create a list of all of FILES and their folder path\n",
    "7. Run a loop, which detects whether the file path and file exists on the github repo (we are going to build it so that it can be pulled down locally)\n",
    "8. Using the notebook export function place each file in its correct place\n",
    "9. Git PULL from VS Code allows us to see IPYNB files in VSCODE. VS Code is mapped to to the Development Environment. Reason: So we can do .py development for packages / wheel files (something that's not possible within Synapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Visible Inside Synapse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cb25bae0a4dfe2e1e5bdb930c477055c2617a8d1ec06ff7e0af720f16e54af2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
