{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "bigDataCluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "99a1dccd-45c2-42db-8029-28b8e2754441"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/4f1bc772-7792-4285-99d9-3463b8d7f994/resourceGroups/synapse-dev-rg/providers/Microsoft.Synapse/workspaces/synapsewsdeploychd/bigDataPools/bigDataCluster",
				"name": "bigDataCluster",
				"type": "Spark",
				"endpoint": "https://synapsewsdeploychd.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bigDataCluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%pip uninstall protobuf\r\n",
					"%pip uninstall protobuf\r\n",
					"%pip uninstall protobuf\r\n",
					"%pip uninstall protobuf\r\n",
					"%pip install --user protobuf==3.20.1\r\n",
					"\r\n",
					"\r\n",
					"import random\r\n",
					"from dstoolkit.core import Log\r\n",
					"from dstoolkit.core import trace\r\n",
					"from dstoolkit.dev import apply_test\r\n",
					"from dstoolkit.dev import DataStep\r\n",
					"from dstoolkit.dev import DataStepDataframe\r\n",
					"from dstoolkit.engine import Engine\r\n",
					"# from pathlib import Path\r\n",
					"# from pydataset import data\r\n",
					"from pyspark.sql import functions as F\r\n",
					"from sklearn import svm\r\n",
					"from sklearn import datasets\r\n",
					"from sklearn.model_selection import cross_val_score\r\n",
					"from sklearn.model_selection import ParameterSampler\r\n",
					"from sklearn.utils.fixes import loguniform\r\n",
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"class Step_loadData(DataStep):\r\n",
					"    \"\"\"Load the defined dataset.\"\"\"\r\n",
					"\r\n",
					"    def test(self):\r\n",
					"        \"\"\"Apply data tests.\"\"\"\r\n",
					"        self.test_is_dataframe_empty(df=self.output_data.dataframe)\r\n",
					"        self.test_null_values(\r\n",
					"            cols=['sepallength', 'sepalwidth'],\r\n",
					"            dt=self.output_data.dataframe\r\n",
					"        )\r\n",
					"\r\n",
					"    @apply_test\r\n",
					"    @trace\r\n",
					"    def initialize(self):\r\n",
					"        \"\"\"\r\n",
					"        Initialize the DataStep.\r\n",
					"\r\n",
					"        Parameters\r\n",
					"        ----------\r\n",
					"        name_dataset : str\r\n",
					"            Name of the dataset to load from pydataset package\r\n",
					"        \"\"\"\r\n",
					"        s_dt = datasets.load_iris()\r\n",
					"        p_df = pd.DataFrame(\r\n",
					"            data=np.c_[s_dt['data'], s_dt['target']],\r\n",
					"            columns=s_dt['feature_names'] + ['target']\r\n",
					"        )\r\n",
					"        p_df.columns = [c.replace(' ', '') for c in p_df.columns]\r\n",
					"        p_df.columns = [c.replace('(cm)', '') for c in p_df.columns]\r\n",
					"        dt = self.spark.createDataFrame(p_df)\r\n",
					"        self.set_output_data(dt)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"class RandomException(Exception):\r\n",
					"    \"\"\"Random exception\"\"\"\r\n",
					"    pass\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"class Step_crossValidate(DataStep):\r\n",
					"    \"\"\"Run multiple models in parallel.\"\"\"\r\n",
					"\r\n",
					"    def test(self):\r\n",
					"        pass\r\n",
					"\r\n",
					"    @trace(attrs_refact=['ai_cs'])\r\n",
					"    def initialize(\r\n",
					"        self,\r\n",
					"        dt: DataStepDataframe,\r\n",
					"        pipeline_name: str,\r\n",
					"        ai_cs: str,\r\n",
					"        n_iter: int\r\n",
					"    ):\r\n",
					"        param_grid = {\r\n",
					"            'C': loguniform(1e0, 1e3),\r\n",
					"            'kernel': ['linear', 'rbf'],\r\n",
					"            'class_weight': ['balanced', None]\r\n",
					"        }\r\n",
					"        rng = np.random.RandomState(0)\r\n",
					"        param_list = list(\r\n",
					"            ParameterSampler(\r\n",
					"                param_grid,\r\n",
					"                n_iter=n_iter,\r\n",
					"                random_state=rng\r\n",
					"            )\r\n",
					"        )\r\n",
					"        # p_dt = Engine.get_instance().spark().createDataFrame(pd.DataFrame(param_list)).\\\r\n",
					"        #     withColumn('id', F.monotonically_increasing_id())\r\n",
					"        p_dt = self.spark.createDataFrame(pd.DataFrame(param_list)).\\\r\n",
					"            withColumn('id', F.monotonically_increasing_id())\r\n",
					"        dt_train = dt.dataframe.crossJoin(\r\n",
					"            p_dt\r\n",
					"        )\r\n",
					"\r\n",
					"        udf_schema = dt_train.select(\r\n",
					"            'id',\r\n",
					"            F.lit(0.0).alias('score')\r\n",
					"        ).schema\r\n",
					"\r\n",
					"        def pudf_train(dt_model):\r\n",
					"            param_id = dt_model['id'].unique()[0]\r\n",
					"            param_c = dt_model['C'].unique()[0]\r\n",
					"            param_class_weight = dt_model['class_weight'].unique()[0]\r\n",
					"            param_kernel = dt_model['kernel'].unique()[0]\r\n",
					"\r\n",
					"            logging_custom_dimensions = {\r\n",
					"                'id': str(param_id),\r\n",
					"                'C': str(param_c),\r\n",
					"                'class_weight': param_class_weight,\r\n",
					"                'kernel': param_kernel\r\n",
					"            }\r\n",
					"\r\n",
					"            Log(pipeline_name, ai_cs)\r\n",
					"\r\n",
					"            try:\r\n",
					"\r\n",
					"                # Raising randomly an exception\r\n",
					"                if random.randint(0, 20) > 15:\r\n",
					"                    raise RandomException('Random exception')\r\n",
					"\r\n",
					"                dt_x = dt_model[\r\n",
					"                    [\r\n",
					"                        'sepallength',\r\n",
					"                        'sepalwidth',\r\n",
					"                        'petallength',\r\n",
					"                        'petalwidth'\r\n",
					"                    ]\r\n",
					"                ]\r\n",
					"                y = dt_model['target']\r\n",
					"                clf = svm.SVC(\r\n",
					"                    kernel=param_kernel,\r\n",
					"                    C=param_c,\r\n",
					"                    class_weight=param_class_weight,\r\n",
					"                    random_state=42\r\n",
					"                )\r\n",
					"                scores = cross_val_score(clf, dt_x, y, cv=5, scoring='f1_macro')\r\n",
					"                score = scores.mean()\r\n",
					"                dt_out = pd.DataFrame(\r\n",
					"                    {\r\n",
					"                        'id': [param_id],\r\n",
					"                        'score': [score]\r\n",
					"                    }\r\n",
					"                )\r\n",
					"                Log.get_instance().log_info(\"Training:success\", custom_dimension=logging_custom_dimensions)\r\n",
					"            except Exception:\r\n",
					"                Log.get_instance().log_error(\"Training:failed\", custom_dimension=logging_custom_dimensions)\r\n",
					"                dt_out = pd.DataFrame(\r\n",
					"                    {\r\n",
					"                        'id': [param_id],\r\n",
					"                        'score': [-1]\r\n",
					"                    }\r\n",
					"                )\r\n",
					"            return dt_out\r\n",
					"\r\n",
					"        '''\r\n",
					"        dt_model = dt_train.where(F.col('id') == 25769804021).toPandas()\r\n",
					"        '''\r\n",
					"        dt_cross_evals = dt_train.\\\r\n",
					"            groupBy(['id']).\\\r\n",
					"            applyInPandas(pudf_train, schema=udf_schema).\\\r\n",
					"            cache()\r\n",
					"        dt_cross_evals.count()\r\n",
					"        self.set_output_data(dt_cross_evals)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"PIPELINE_NAME = \"Remote Testing\"\r\n",
					"ENVVAR_AI_CS = 'AI_CS'\r\n",
					"KEY_VAULT_SECRET = 'aics'\r\n",
					"KEY_VAULT_NAME = 'synapseaccleratorkv'\r\n",
					"RUN_ID = 'test_run_id'\r\n",
					"\r\n",
					"Engine()\r\n",
					"Engine().get_instance().initialize_env()\r\n",
					"# pipeline_name = Path(__file__).stem\r\n",
					"\r\n",
					"if Engine().get_instance().is_ide_local():\r\n",
					"    from dotenv import load_dotenv\r\n",
					"    import os\r\n",
					"    load_dotenv(dotenv_path='/workspaces/dstoolkit-ml-ops-for-synapse-dev/.env')\r\n",
					"    Engine().get_instance().initialize_logger(\r\n",
					"        pipeline_name=PIPELINE_NAME,\r\n",
					"        aics=os.environ[ENVVAR_AI_CS]\r\n",
					"    )\r\n",
					"elif Engine().get_instance().is_ide_remote():\r\n",
					"    Engine().get_instance().initialize_logger(\r\n",
					"        pipeline_name=PIPELINE_NAME,\r\n",
					"        aics_kv_secret=KEY_VAULT_SECRET, \r\n",
					"        aics_kv_name=KEY_VAULT_NAME\r\n",
					"    )\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"step_loadData = Step_loadData(\r\n",
					"    spark=Engine.get_instance().spark(),\r\n",
					"    run_id=RUN_ID\r\n",
					")\r\n",
					"\r\n",
					"step_loadData.initialize()\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"step_crossValidate = Step_crossValidate(\r\n",
					"    spark=Engine.get_instance().spark(),\r\n",
					"    run_id=RUN_ID\r\n",
					")\r\n",
					"\r\n",
					"step_crossValidate.initialize(\r\n",
					"    dt=step_loadData.output_data,\r\n",
					"    pipeline_name=PIPELINE_NAME,\r\n",
					"    ai_cs=Engine().get_instance().ai_cs,\r\n",
					"    n_iter=1000\r\n",
					")\r\n",
					"\r\n",
					"step_crossValidate.output_data.dataframe.toPandas()"
				],
				"execution_count": null
			}
		]
	}
}