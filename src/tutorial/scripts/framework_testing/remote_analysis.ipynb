{
    "cells": [
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "An error was encountered:\n",
         "Invalid status code '401' from https://synapseaccleratorsws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool/sessions/-1 with error payload: {\"code\":\"ExpiredAuthenticationToken\",\"message\":\"Token Authentication failed with SecurityTokenExpiredException - IDX10223: Lifetime validation failed. The token is expired. ValidTo: '[PII is hidden]', Current time: '[PII is hidden]'.\"}\n"
        ]
       }
      ],
      "source": [
       "import random\n",
       "from dstoolkit.core import Log\n",
       "from dstoolkit.core import trace\n",
       "from dstoolkit.dev import apply_test\n",
       "from dstoolkit.dev import DataStep\n",
       "from dstoolkit.dev import DataStepDataframe\n",
       "from dstoolkit.engine import Engine\n",
       "# from pathlib import Path\n",
       "# from pydataset import data\n",
       "from pyspark.sql import functions as F\n",
       "from sklearn import svm\n",
       "from sklearn import datasets\n",
       "from sklearn.model_selection import cross_val_score\n",
       "from sklearn.model_selection import ParameterSampler\n",
       "from sklearn.utils.fixes import loguniform\n",
       "import numpy as np\n",
       "import pandas as pd"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "\n",
       "class Step_loadData(DataStep):\n",
       "    \"\"\"Load the defined dataset.\"\"\"\n",
       "\n",
       "    def test(self):\n",
       "        \"\"\"Apply data tests.\"\"\"\n",
       "        self.test_is_dataframe_empty(df=self.output_data.dataframe)\n",
       "        self.test_null_values(\n",
       "            cols=['sepallength', 'sepalwidth'],\n",
       "            dt=self.output_data.dataframe\n",
       "        )\n",
       "\n",
       "    @apply_test\n",
       "    @trace\n",
       "    def initialize(self):\n",
       "        \"\"\"\n",
       "        Initialize the DataStep.\n",
       "\n",
       "        Parameters\n",
       "        ----------\n",
       "        name_dataset : str\n",
       "            Name of the dataset to load from pydataset package\n",
       "        \"\"\"\n",
       "        s_dt = datasets.load_iris()\n",
       "        p_df = pd.DataFrame(\n",
       "            data=np.c_[s_dt['data'], s_dt['target']],\n",
       "            columns=s_dt['feature_names'] + ['target']\n",
       "        )\n",
       "        p_df.columns = [c.replace(' ', '') for c in p_df.columns]\n",
       "        p_df.columns = [c.replace('(cm)', '') for c in p_df.columns]\n",
       "        dt = self.spark.createDataFrame(p_df)\n",
       "        self.set_output_data(dt)\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class RandomException(Exception):\n",
       "    \"\"\"Random exception\"\"\"\n",
       "    pass"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class Step_crossValidate(DataStep):\n",
       "    \"\"\"Run multiple models in parallel.\"\"\"\n",
       "\n",
       "    def test(self):\n",
       "        pass\n",
       "\n",
       "    @trace(attrs_refact=['ai_cs'])\n",
       "    def initialize(\n",
       "        self,\n",
       "        dt: DataStepDataframe,\n",
       "        pipeline_name: str,\n",
       "        ai_cs: str,\n",
       "        n_iter: int\n",
       "    ):\n",
       "        param_grid = {\n",
       "            'C': loguniform(1e0, 1e3),\n",
       "            'kernel': ['linear', 'rbf'],\n",
       "            'class_weight': ['balanced', None]\n",
       "        }\n",
       "        rng = np.random.RandomState(0)\n",
       "        param_list = list(\n",
       "            ParameterSampler(\n",
       "                param_grid,\n",
       "                n_iter=n_iter,\n",
       "                random_state=rng\n",
       "            )\n",
       "        )\n",
       "        # p_dt = Engine.get_instance().spark().createDataFrame(pd.DataFrame(param_list)).\\\n",
       "        #     withColumn('id', F.monotonically_increasing_id())\n",
       "        p_dt = self.spark.createDataFrame(pd.DataFrame(param_list)).\\\n",
       "            withColumn('id', F.monotonically_increasing_id())\n",
       "        dt_train = dt.dataframe.crossJoin(\n",
       "            p_dt\n",
       "        )\n",
       "\n",
       "        udf_schema = dt_train.select(\n",
       "            'id',\n",
       "            F.lit(0.0).alias('score')\n",
       "        ).schema\n",
       "\n",
       "        def pudf_train(dt_model):\n",
       "            param_id = dt_model['id'].unique()[0]\n",
       "            param_c = dt_model['C'].unique()[0]\n",
       "            param_class_weight = dt_model['class_weight'].unique()[0]\n",
       "            param_kernel = dt_model['kernel'].unique()[0]\n",
       "\n",
       "            logging_custom_dimensions = {\n",
       "                'id': str(param_id),\n",
       "                'C': str(param_c),\n",
       "                'class_weight': param_class_weight,\n",
       "                'kernel': param_kernel\n",
       "            }\n",
       "\n",
       "            Log(pipeline_name, ai_cs)\n",
       "\n",
       "            try:\n",
       "\n",
       "                # Raising randomly an exception\n",
       "                if random.randint(0, 20) > 15:\n",
       "                    raise RandomException('Random exception')\n",
       "\n",
       "                dt_x = dt_model[\n",
       "                    [\n",
       "                        'sepallength',\n",
       "                        'sepalwidth',\n",
       "                        'petallength',\n",
       "                        'petalwidth'\n",
       "                    ]\n",
       "                ]\n",
       "                y = dt_model['target']\n",
       "                clf = svm.SVC(\n",
       "                    kernel=param_kernel,\n",
       "                    C=param_c,\n",
       "                    class_weight=param_class_weight,\n",
       "                    random_state=42\n",
       "                )\n",
       "                scores = cross_val_score(clf, dt_x, y, cv=5, scoring='f1_macro')\n",
       "                score = scores.mean()\n",
       "                dt_out = pd.DataFrame(\n",
       "                    {\n",
       "                        'id': [param_id],\n",
       "                        'score': [score]\n",
       "                    }\n",
       "                )\n",
       "                Log.get_instance().log_info(\"Training:success\", custom_dimension=logging_custom_dimensions)\n",
       "            except Exception:\n",
       "                Log.get_instance().log_error(\"Training:failed\", custom_dimension=logging_custom_dimensions)\n",
       "                dt_out = pd.DataFrame(\n",
       "                    {\n",
       "                        'id': [param_id],\n",
       "                        'score': [-1]\n",
       "                    }\n",
       "                )\n",
       "            return dt_out\n",
       "\n",
       "        '''\n",
       "        dt_model = dt_train.where(F.col('id') == 25769804021).toPandas()\n",
       "        '''\n",
       "        dt_cross_evals = dt_train.\\\n",
       "            groupBy(['id']).\\\n",
       "            applyInPandas(pudf_train, schema=udf_schema).\\\n",
       "            cache()\n",
       "        dt_cross_evals.count()\n",
       "        self.set_output_data(dt_cross_evals)\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "PIPELINE_NAME = \"Remote Testing\"\n",
       "ENVVAR_AI_CS = 'AI_CS'\n",
       "KEY_VAULT_SECRET = 'aics'\n",
       "KEY_VAULT_NAME = 'synapseaccleratorkv'\n",
       "RUN_ID = 'test_run_id'\n",
       "\n",
       "Engine()\n",
       "Engine().get_instance().initialize_env()\n",
       "# pipeline_name = Path(__file__).stem\n",
       "\n",
       "if Engine().get_instance().is_ide_local():\n",
       "    from dotenv import load_dotenv\n",
       "    import os\n",
       "    load_dotenv(dotenv_path='/workspaces/dstoolkit-ml-ops-for-synapse-dev/.env')\n",
       "    Engine().get_instance().initialize_logger(\n",
       "        pipeline_name=PIPELINE_NAME,\n",
       "        aics=os.environ[ENVVAR_AI_CS]\n",
       "    )\n",
       "elif Engine().get_instance().is_ide_remote():\n",
       "    Engine().get_instance().initialize_logger(\n",
       "        pipeline_name=PIPELINE_NAME,\n",
       "        aics_kv_secret=KEY_VAULT_SECRET, \n",
       "        aics_kv_name=KEY_VAULT_NAME\n",
       "    )"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "step_loadData = Step_loadData(\n",
       "    spark=Engine.get_instance().spark(),\n",
       "    run_id=RUN_ID\n",
       ")\n",
       "\n",
       "step_loadData.initialize()\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "step_crossValidate = Step_crossValidate(\n",
       "    spark=Engine.get_instance().spark(),\n",
       "    run_id=RUN_ID\n",
       ")\n",
       "\n",
       "step_crossValidate.initialize(\n",
       "    dt=step_loadData.output_data,\n",
       "    pipeline_name=PIPELINE_NAME,\n",
       "    ai_cs=Engine().get_instance().ai_cs,\n",
       "    n_iter=1000\n",
       ")\n",
       "\n",
       "step_crossValidate.output_data.dataframe.toPandas()"
      ]
     }
    ],
    "metadata": {
     "interpreter": {
      "hash": "5af57b14ec6ff2ad1ba821c600b50477f0fd632d93e4bdf5b888d7e5f16223c3"
     },
     "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('hdinsightJupyter': venv)",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "python",
       "version": 3
      },
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "python3",
      "version": "3.8.12"
     },
     "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   